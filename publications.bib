
@article{wuAnalyticSphericalHarmonic2020,
	title = {Analytic spherical harmonic gradients for real-time rendering with many polygonal area lights},
	volume = {39},
	copyright = {All rights reserved},
	issn = {0730-0301, 1557-7368},
	url = {https://dl.acm.org/doi/10.1145/3386569.3392373},
	doi = {10.1145/3386569.3392373},
	abstract = {Recent work has developed analytic formulae for spherical harmonic (SH) coefficients from uniform polygonal lights, enabling near-field area lights to be included in Precomputed Radiance Transfer (PRT) systems, and in offline rendering. However, the method is inefficient since coefficients need to be recomputed at each vertex or shading point, for each light, even though the SH coefficients vary smoothly in space. The complexity scales linearly with the number of lights, making many-light rendering difficult. In this paper, we develop a
              novel analytic formula for the spatial gradients of the spherical harmonic coefficients for uniform polygonal area lights.
              The result is a significant generalization, involving the Reynolds transport theorem to reduce the problem to a boundary integral for which we derive a new analytic formula, showing how to reduce a key term to an earlier recurrence for SH coefficients. The implementation requires only minor additions to existing code for SH coefficients. The results also hold implications for recent efforts on differentiable rendering. We show that SH gradients enable very sparse spatial sampling, followed by accurate Hermite interpolation. This enables
              scaling PRT to hundreds of area lights
              with minimal overhead and real-time frame rates. Moreover, the SH gradient formula is a new mathematical result that potentially enables many other graphics applications.},
	language = {en},
	number = {4},
	urldate = {2024-02-01},
	journal = {ACM Transactions on Graphics},
	author = {Wu, Lifan and Cai, Guangyan and Zhao, Shuang and Ramamoorthi, Ravi},
	month = aug,
	year = {2020},
	keywords = {differentiable rendering},
	file = {Full Text PDF:C\:\\Users\\NickCai\\Zotero\\storage\\Z5PSLWHU\\Wu et al. - 2020 - Analytic spherical harmonic gradients for real-tim.pdf:application/pdf},
}

@article{caiPhysicsBasedInverse2022,
	title = {Physics‐{Based} {Inverse} {Rendering} using {Combined} {Implicit} and {Explicit} {Geometries}},
	volume = {41},
	copyright = {All rights reserved},
	url = {https://par.nsf.gov/biblio/10443938-physicsbased-inverse-rendering-using-combined-implicit-explicit-geometries},
	doi = {10.1111/cgf.14592},
	abstract = {Abstract Mathematically representing the shape of an object is a key ingredient for solving inverse rendering problems. Explicit representations like meshes are efficient to render in a differentiable fashion but have difficulties handling topology changes. Implicit representations like signed‐distance functions, on the other hand, offer better support of topology changes but are much more difficult to use for physics‐based differentiable rendering. We introduce a new physics‐based inverse rendering pipeline that uses both implicit and explicit representations. Our technique enjoys the benefit of both representations by supporting both topology changes and differentiable rendering of complex effects such as environmental illumination, soft shadows, and interreflection. We demonstrate the effectiveness of our technique using several synthetic and real examples.},
	language = {English},
	number = {4},
	urldate = {2024-02-01},
	journal = {Computer Graphics Forum},
	author = {Cai, G. and Yan, K. and Dong, Z. and Gkioulekas, I. and Zhao, S.},
	month = jul,
	year = {2022},
	note = {Publisher: Wiley-Blackwell},
	keywords = {differentiable rendering, inverse rendering, 3D reconstruction},
	file = {Submitted Version:C\:\\Users\\NickCai\\Zotero\\storage\\C3HL249Z\\Cai et al. - 2022 - Physics‐Based Inverse Rendering using Combined Imp.pdf:application/pdf},
}

@article{wuDifferentiableTimegatedRendering2021,
	title = {Differentiable time-gated rendering},
	volume = {40},
	copyright = {All rights reserved},
	issn = {0730-0301},
	url = {https://dl.acm.org/doi/10.1145/3478513.3480489},
	doi = {10.1145/3478513.3480489},
	abstract = {The continued advancements of time-of-flight imaging devices have enabled new imaging pipelines with numerous applications. Consequently, several forward rendering techniques capable of accurately and efficiently simulating these devices have been introduced. However, general-purpose differentiable rendering techniques that estimate derivatives of time-of-flight images are still lacking. In this paper, we introduce a new theory of differentiable time-gated rendering that enjoys the generality of differentiating with respect to arbitrary scene parameters. Our theory also allows the design of advanced Monte Carlo estimators capable of handling cameras with near-delta or discontinuous time gates. We validate our theory by comparing derivatives generated with our technique and finite differences. Further, we demonstrate the usefulness of our technique using a few proof-of-concept inverse-rendering examples that simulate several time-of-flight imaging scenarios.},
	number = {6},
	urldate = {2024-02-01},
	journal = {ACM Transactions on Graphics},
	author = {Wu, Lifan and Cai, Guangyan and Ramamoorthi, Ravi and Zhao, Shuang},
	month = dec,
	year = {2021},
	keywords = {differentiable rendering, non-line-of-sight reconstruction},
	pages = {287:1--287:16},
	file = {Full Text PDF:C\:\\Users\\NickCai\\Zotero\\storage\\IKALHZIG\\Wu et al. - 2021 - Differentiable time-gated rendering.pdf:application/pdf},
}

@inproceedings{sunNeuralPBIRReconstructionShape2023,
	title = {Neural-{PBIR} {Reconstruction} of {Shape}, {Material}, and {Illumination}},
	copyright = {All rights reserved},
	url = {https://ieeexplore.ieee.org/document/10376802},
	doi = {10.1109/ICCV51070.2023.01654},
	abstract = {Reconstructing the shape and spatially varying surface appearances of a physical-world object as well as its surrounding illumination based on 2D images (e.g., photographs) of the object has been a long-standing problem in computer vision and graphics. In this paper, we introduce an accurate and highly efficient object reconstruction pipeline combining neural based object reconstruction and physics-based inverse rendering (PBIR). Our pipeline firstly leverages a neural SDF based shape reconstruction to produce high-quality but potentially imperfect object shape. Then, we introduce a neural material and lighting distillation stage to achieve high-quality predictions for material and illumination. In the last stage, initialized by the neural predictions, we perform PBIR to refine the initial results and obtain the final high-quality reconstruction of object shape, material, and illumination. Experimental results demonstrate our pipeline significantly outperforms existing methods quality-wise and performance-wise. Code: https://neural-pbir.github.io/},
	urldate = {2024-05-06},
	booktitle = {2023 {IEEE}/{CVF} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	author = {Sun, Cheng and Cai, Guangyan and Li, Zhengqin and Yan, Kai and Zhang, Cheng and Marshall, Carl and Huang, Jia-Bin and Zhao, Shuang and Dong, Zhao},
	month = oct,
	year = {2023},
	note = {ISSN: 2380-7504},
	keywords = {differentiable rendering, inverse rendering, 3D reconstruction, NeRF},
	pages = {18000--18010},
	file = {IEEE Xplore Full Text PDF:C\:\\Users\\NickCai\\Zotero\\storage\\SIB33CQB\\Sun et al. - 2023 - Neural-PBIR Reconstruction of Shape, Material, and.pdf:application/pdf},
}

@misc{caiPBIRNIEGlossyObject2024,
	title = {{PBIR}-{NIE}: {Glossy} {Object} {Capture} under {Non}-{Distant} {Lighting}},
	shorttitle = {{PBIR}-{NIE}},
	url = {http://arxiv.org/abs/2408.06878},
	doi = {10.48550/arXiv.2408.06878},
	abstract = {Glossy objects present a significant challenge for 3D reconstruction from multi-view input images under natural lighting. In this paper, we introduce PBIR-NIE, an inverse rendering framework designed to holistically capture the geometry, material attributes, and surrounding illumination of such objects. We propose a novel parallax-aware non-distant environment map as a lightweight and efficient lighting representation, accurately modeling the near-field background of the scene, which is commonly encountered in real-world capture setups. This feature allows our framework to accommodate complex parallax effects beyond the capabilities of standard infinite-distance environment maps. Our method optimizes an underlying signed distance field (SDF) through physics-based differentiable rendering, seamlessly connecting surface gradients between a triangle mesh and the SDF via neural implicit evolution (NIE). To address the intricacies of highly glossy BRDFs in differentiable rendering, we integrate the antithetic sampling algorithm to mitigate variance in the Monte Carlo gradient estimator. Consequently, our framework exhibits robust capabilities in handling glossy object reconstruction, showcasing superior quality in geometry, relighting, and material estimation.},
	urldate = {2024-09-28},
	publisher = {arXiv},
	author = {Cai, Guangyan and Luan, Fujun and Hašan, Miloš and Zhang, Kai and Bi, Sai and Xu, Zexiang and Georgiev, Iliyan and Zhao, Shuang},
	month = aug,
	year = {2024},
	note = {arXiv:2408.06878 [cs]},
	file = {arXiv Fulltext PDF:C\:\\Users\\NickCai\\Zotero\\storage\\YD49QRBF\\Cai et al. - 2024 - PBIR-NIE Glossy Object Capture under Non-Distant Lighting.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\NickCai\\Zotero\\storage\\KSVUSQ3K\\2408.html:text/html},
}
